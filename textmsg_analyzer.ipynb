{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matchten/textmsg-analyzer/blob/main/textmsg_analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eO6Y_5t2_RC"
      },
      "outputs": [],
      "source": [
        "# Import Packages\n",
        "!pip install transformers\n",
        "!pip install datasets --upgrade\n",
        "!pip install evaluate\n",
        "!pip install transformers[torch]\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY7tUIAnD-YC"
      },
      "outputs": [],
      "source": [
        "# Load original dataset\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "from datasets import concatenate_datasets, load_dataset\n",
        "from datasets import Dataset, DatasetDict\n",
        "import numpy as np\n",
        "\n",
        "new_data = load_dataset(\"conv_ai_2\")\n",
        "\n",
        "# Preprocess new dataset to convert input into string, not dict\n",
        "\n",
        "new_data = new_data.select_columns([\"dialog\", \"eval_score\"])\n",
        "new_data = new_data.rename_column(\"dialog\", \"text\")\n",
        "new_data = new_data.rename_column(\"eval_score\", \"label\")\n",
        "new_data = new_data.filter(lambda x: x[\"label\"] >= 0)\n",
        "\n",
        "\n",
        "def dict_to_string(example):\n",
        "    text = \"\\\"\"\n",
        "    for message in example[\"text\"]:\n",
        "        if message[\"sender_class\"] == \"Human\":\n",
        "            text += message[\"text\"] + \" \"\n",
        "    text = text[:-1]\n",
        "    text += \"\\\"\"\n",
        "    example[\"text\"] = text\n",
        "    return example\n",
        "\n",
        "def normalize_labels(example):\n",
        "    # 0 -> negative, 1 -> neutral, 2 -> positive\n",
        "    score = example[\"label\"]\n",
        "    if score < 3:\n",
        "        example[\"label\"] = 0\n",
        "    elif score == 3:\n",
        "        example[\"label\"] = 1\n",
        "    else:\n",
        "        example[\"label\"] = 2\n",
        "    return example\n",
        "\n",
        "new_data = new_data.map(dict_to_string)\n",
        "new_data = new_data.map(normalize_labels)\n",
        "\n",
        "new_data = new_data[\"train\"].train_test_split(test_size = 0.1)\n",
        "\n",
        "# print(new_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional finetune dataset consisting of daily dialogue and emotion sequences\n",
        "additional_data = load_dataset(\"daily_dialog\")\n",
        "additional_data = additional_data.select_columns([\"dialog\", \"emotion\"])\n",
        "additional_data = additional_data.rename_column(\"dialog\", \"text\")\n",
        "additional_data = additional_data.rename_column(\"emotion\", \"label\")\n",
        "\n",
        "def list_to_string(example):\n",
        "    text = \"\"\n",
        "    for message in example[\"text\"]:\n",
        "        text += message\n",
        "    example[\"text\"] = text\n",
        "    return example\n",
        "\n",
        "def normalize_labels(example):\n",
        "    emotion_sequence = example[\"label\"]\n",
        "    for i in range(len(emotion_sequence)):\n",
        "        emotion = emotion_sequence[i]\n",
        "        if emotion == 1 or 2 or 3 or 5:\n",
        "            emotion_sequence[i] = 0\n",
        "        elif emotion == 0:\n",
        "            emotion_sequence[i] = 1\n",
        "        else:\n",
        "            emotion_sequence[i] = 2\n",
        "\n",
        "    normalized_emotion = round(sum(emotion for emotion in emotion_sequence)/len(emotion_sequence))\n",
        "    example[\"label\"] = np.int32(normalized_emotion)\n",
        "    return example\n",
        "\n",
        "additional_data = additional_data.map(list_to_string)\n",
        "additional_data = additional_data.map(normalize_labels)\n",
        "additional_data[\"test\"] = concatenate_datasets([additional_data[\"test\"], additional_data[\"validation\"]])\n",
        "del additional_data[\"validation\"]\n",
        "\n",
        "# print(additional_data)"
      ],
      "metadata": {
        "id": "XxtTdB65MDIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the datasets to form a finetuned dataset\n",
        "finetune_train = concatenate_datasets([new_data[\"train\"], additional_data[\"train\"]])\n",
        "finetune_test = concatenate_datasets([new_data[\"test\"], additional_data[\"test\"]])\n",
        "\n",
        "# print(finetune_train)\n",
        "# print(finetune_test)"
      ],
      "metadata": {
        "id": "4YJ9Ks8FYZHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shV_NIaNFHmD"
      },
      "outputs": [],
      "source": [
        "# Tokenizing data\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "\n",
        "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels = 3)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length = 512, return_tensors = 'pt')\n",
        "\n",
        "tokenized_finetune_train = finetune_train.map(tokenize_function, batched=True)\n",
        "tokenized_finetune_test = finetune_test.map(tokenize_function, batched=True)\n",
        "\n",
        "# print(tokenized_finetune_train)\n",
        "# print(tokenized_finetune_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "n6KEpqSnHHzy"
      },
      "outputs": [],
      "source": [
        "# Creating Model\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import accelerate\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"test_trainer\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size = 15,\n",
        "    per_device_eval_batch_size = 8,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_finetune_train,\n",
        "    eval_dataset=tokenized_finetune_test,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gBisMP2U8fO"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "18P8u2fca7-ZoYEwvdH2Lhubbs0othJsI",
      "authorship_tag": "ABX9TyNEvDW5XBWS/d8NZfUA8Pie",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}